{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "TODO : Add intro to usecase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now using the data used here : https://github.com/shinchan75034/LSTM_TouchPoint.  \n",
    "        \n",
    "Eventually would want to use data at http://archive.ics.uci.edu/ml/datasets/Online+Retail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : Explain data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow this sequence in this notebook\n",
    "\n",
    "1. Execute the python script that builds and trains the LSTM model using Keras. \n",
    "2. Execute training the Amazon SageMaker's 'local' mode\n",
    "3. Execute training on a training cluster of ML instances managed by Amazon SageMaker\n",
    "4. Deploy the trained model.\n",
    "5. Execute inferences agains the deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#s3 = boto3.resource('s3') \n",
    "#s3.Bucket(\"555360056434-sagemaker-us-east-1\").download_file('small_train_data_orig.txt', 'data.txt')\n",
    "\n",
    "data_file=\"data.txt\"\n",
    "\n",
    "with open(data_file, 'r') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        data = np.array(lines)\n",
    "        print(\"Data : \", data)\n",
    "\n",
    "        ##Split data into training (60%), validation (20%) and test (20%) data sets\n",
    "        train_dat ,remained = train_test_split(data,test_size=0.4)\n",
    "        validation_dat, test_dat = train_test_split(remained, test_size = 0.5)\n",
    "\n",
    "        #Verify training, validation and test data set sizes\n",
    "        print(\"\\nTraining Data Size \", len(train_dat))\n",
    "        print(\"Validation Data Size \", len(validation_dat))\n",
    "        print(\"Test Data Size \", len(test_dat))\n",
    "        \n",
    "#convert training, validation and test data sets to lists\n",
    "train_lines = list(train_dat)\n",
    "validation_lines = list(validation_dat)\n",
    "test_lines = list(test_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility methods to create corpus, split lines into input & target and encode the data\n",
    "\n",
    "#Create the corpus dictionary\n",
    "def create_corpus_dict(word_list):\n",
    "  token_index = dict(\n",
    "    [(word, i) for i, word in enumerate(word_list)])\n",
    "  return token_index\n",
    "\n",
    "#Split a given line into input and target\n",
    "#TODO : Can this be made generic enough to use in the above cell to build corpus\n",
    "def split_input_and_target(line_list):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    try:\n",
    "\n",
    "        for line in line_list:\n",
    "            _, input_text, target_text = line.split('\\t')\n",
    "            # We use \"tab\" as the \"start sequence\" character\n",
    "            # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "            target_text = '<start>' + \" \" + target_text + \" \" + '<stop>' \n",
    "            input_texts.append(input_text)\n",
    "            target_texts.append(target_text)\n",
    "            \n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    return input_texts, target_texts\n",
    "\n",
    "## Method to encode data\n",
    "\n",
    "def encode_data(input_texts,target_texts, input_vocab, target_vocab, input_corpus, target_corpus) :\n",
    "    \n",
    "    #Get the array/list length/counts\n",
    "    # input and target may have different vocab and different token count.\n",
    "    input_vocab = sorted(list(input_vocab))\n",
    "    target_vocab = sorted(list(target_vocab))\n",
    "    num_encoder_tokens = len(input_vocab)\n",
    "    num_decoder_tokens = len(target_vocab)\n",
    "    max_encoder_seq_length = max([len(txt.split()) for txt in input_texts]) # number of words in each string.  Use max length to make all sequences same size.\n",
    "    max_decoder_seq_length = max([len(txt.split()) for txt in target_texts])\n",
    "    \n",
    "    #Create zero encoded/decoder arrays of correct size\n",
    "    encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "    decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "    decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "    \n",
    "    #Now update the encoded/decoded arrays with 1.\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, word in enumerate(input_text.split()):\n",
    "            encoder_input_data[i, t, input_corpus[word]] = 1.\n",
    "        for t, word in enumerate(target_text.split()):\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t, target_corpus[word]] = 1.\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_corpus[word]] = 1.\n",
    "    \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Utility methods to upload to and read from S3 bucket.\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "s3 = boto3.resource('s3') \n",
    "s3.Bucket(\"555360056434-sagemaker-us-east-1\").download_file('small_train_data_orig.txt', 'data.txt')\n",
    "\n",
    "##Save ndarray to file, upload to S3\n",
    "def upload_ndarray_to_s3(encoder_input_data, s3_prefix):\n",
    "    local_file = 'encoded_data.npy'\n",
    "    #s3_prefix = 'test/encoder_input_data.npy'\n",
    "    np.save(local_file, encoder_input_data) \n",
    "    s3.Bucket('555360056434-sagemaker-us-east-1').upload_file(local_file, s3_prefix)\n",
    "    \n",
    "    \n",
    "### Read ndarray from S3\n",
    "def read_ndarray_from_s3(s3_prefix):\n",
    "    local_file_downloaded = 'downloaded_encoder_data.npy'\n",
    "    s3.Bucket(\"555360056434-sagemaker-us-east-1\").download_file(s3_prefix, local_file_downloaded)\n",
    "    downloaded_encoder_input_data = np.load(local_file_downloaded)\n",
    "    return downloaded_encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up all data to build a corpus\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_words = set()\n",
    "target_words = set()\n",
    "\n",
    "for line in lines:\n",
    "      try:\n",
    "        #print (\"line :\", line)\n",
    "        #Split each line into input text and target text.  Ignore the user_id (???)\n",
    "        _, input_text, target_text = line.split(\"\\t\")\n",
    "        #print(\"input_text :\", input_text, \" output text : \", target_text)\n",
    "\n",
    "\n",
    "        # We use \"tab\" as the \"start sequence\" character\n",
    "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "        #Update target text to include <start> and <stop> tokens\n",
    "        target_text = '<start>' + \" \" + target_text + \" \" + '<stop>'   \n",
    "\n",
    "\n",
    "        #Append input_texts and target_texts\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "\n",
    "        #Split the input_text and target_text into words and populate the input_words and target_words\n",
    "        for word in input_text.split():\n",
    "            if word not in input_words:\n",
    "                input_words.add(word)\n",
    "        for word in target_text.split():\n",
    "            if word not in target_words:\n",
    "                target_words.add(word)\n",
    "      except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "#Show what input_text or target_text looks like??\n",
    "\n",
    "print(\"Number of input words \", len(input_words))\n",
    "print(\"Number of target words \", len(target_words))  #Should be two more than input words, since we added <start> and <stop>\n",
    "    \n",
    "    \n",
    "#Build the vocabulary.  Here it is simply union of the input and target words.\n",
    "vocab = list(set(input_words).union(set(target_words)))\n",
    "print(\"Vocab size \", len(vocab))\n",
    "    \n",
    "corpus_dict = create_corpus_dict(vocab)\n",
    "print(\"corpus_dict size \", len(corpus_dict))\n",
    "    \n",
    "# split each set of lines into input and target separately.\n",
    "train_input_texts, train_target_texts  = split_input_and_target(train_lines)\n",
    "#validation_input_texts, validation_target_texts  = split_input_and_target(validation_lines)\n",
    "test_input_texts, test_target_texts  = split_input_and_target(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encode training data and persist to S3\n",
    "train_encoder_input_data, train_decoder_input_data, train_decoder_target_data = encode_data(train_input_texts,train_target_texts, vocab,vocab, corpus_dict, corpus_dict)\n",
    "\n",
    "upload_ndarray_to_s3(train_encoder_input_data, \"train/train_encoder_input_data.npy\")\n",
    "upload_ndarray_to_s3(train_decoder_input_data, \"train/train_decoder_input_data.npy\")\n",
    "upload_ndarray_to_s3(train_decoder_target_data, \"train/train_decoder_target_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encode test data and persist to S3\n",
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = encode_data(test_input_texts,test_target_texts, vocab,vocab, corpus_dict, corpus_dict)\n",
    "\n",
    "upload_ndarray_to_s3(test_encoder_input_data, \"test/test_encoder_input_data.npy\")\n",
    "upload_ndarray_to_s3(test_decoder_input_data, \"test/test_decoder_input_data.npy\")\n",
    "upload_ndarray_to_s3(test_decoder_target_data, \"test/test_decoder_target_data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, execute the python script locally to test it out\n",
    "In this step we can test for syntax and overall script flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete this directory the model is already persisted in this directory.\n",
    "!rm -rf /tmp/model/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python customer_event_prediction_lstm_keras_model.py --epochs 2 --batchsize 64 --modeldir '/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next train using the local mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import time\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "tf_estimator_local = TensorFlow(entry_point='customer_event_prediction_lstm_keras_model.py', \n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='local',\n",
    "                          framework_version='1.12', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={ 'epochs': 1 }\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator_local.fit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a training cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator_on_cluster = TensorFlow(entry_point='customer_event_prediction_lstm_keras_model.py', \n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='ml.m5.xlarge',\n",
    "                          framework_version='1.12', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={\n",
    "                              'epochs': 20,\n",
    "                              'batch-size': 256,\n",
    "                              'learning-rate': 0.01}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf_estimator_on_cluster.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deploy the model trained on the cluster.\n",
    "## This takes approximately XXX minutes\n",
    "\n",
    "tf_endpoint_name = 'customer-event-prediction-lstm'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "tf_predictor = tf_estimator_on_cluster.deploy(initial_instance_count=1,\n",
    "                         instance_type='ml.c5.large',        \n",
    "                         endpoint_name=tf_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets use the deployed model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Method to predict\n",
    "stop_word_list = ['<start>', '<stop>']\n",
    "\n",
    "def predict(prediction_index):\n",
    "    #print(\"encode input data type : \" , type(test_encoder_input_data[prediction_index]), \"encode input data size : \" , len(test_encoder_input_data[prediction_index ]))\n",
    "    predictions_from_model = tf_predictor.predict({'encoder_input_data' : test_encoder_input_data[prediction_index], \n",
    "                                          'decoder_input_data' : test_decoder_input_data[prediction_index]})\n",
    "\n",
    "    #print(\"predictions_from_model \", type(predictions_from_model))\n",
    "    \n",
    "    output_tokens = np.asarray(predictions_from_model['predictions'])\n",
    "    integer_list = output_tokens.argmax(axis=2)\n",
    "\n",
    "    #print(\"output_token.shape \", output_tokens.shape) # (observations, max sequence length, onehot corpus size)\n",
    "    #print(\"integer_list.shape \", integer_list.shape)\n",
    "\n",
    "    # Reassign variables for convenience\n",
    "    input_token_index = corpus_dict\n",
    "    #target_token_index = corpus_dict\n",
    "    \n",
    "    # Reverse-lookup token index to decode sequences back to something readable.\n",
    "    reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "    #reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "    translated_array = np.vectorize(reverse_input_char_index.get)(integer_list) \n",
    "    #translated_array = np.vectorize(reverse_target_char_index.get)(integer_list) \n",
    "\n",
    "    translated_list = translated_array.tolist()\n",
    "    \n",
    "    predicted_sequence = translated_list[0]\n",
    "    \n",
    "    ##Remove <start>, <stop> tokens from the predicted sequence\n",
    "    predicted_sequence_cleaned = [item for item in predicted_sequence if item not in stop_word_list]\n",
    "\n",
    "    return predicted_sequence_cleaned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Predict output for a single element (represented by the index) in the text target \n",
    "predicted_sequence = predict(0)\n",
    "print(\"predicted sequence \", predicted_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now predict for multiple elements in the text target \n",
    "\n",
    "all_predictions = []\n",
    "for i in range(0,len(test_encoder_input_data.tolist())):\n",
    "#for i in range(0,10000):\n",
    "    predicted_sequence = predict(i)\n",
    "    all_predictions.append(predicted_sequence)\n",
    "        \n",
    "print(\"Total number of predictions \", len(all_predictions))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total test target_texts : \", len(test_target_texts))\n",
    "\n",
    "matching_prediction_count = 0\n",
    "\n",
    "for i in range(0,9999):\n",
    "    if (\"visit\" in all_predictions[i] and \"visit\" in test_target_texts[i]):\n",
    "        matching_prediction_count = matching_prediction_count + 1\n",
    "        \n",
    "print(\"matching_prediction_count : \", matching_prediction_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Confustion matrix calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO : Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
