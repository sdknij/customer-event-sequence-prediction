{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Event (Touch Point) Prediction\n",
    "_**Predict a customer's future event or touchpoint using LSTM based on the past events customer participated in.**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "1. [Predict](#Predict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "This notebook illustrates an LSTM based approach to predict customerâ€™s future events based on the past events customer participated in.  Notebook is based on the session \"A novel adoption of LSTM in customer touchpoint prediction\" from Strata 2018 conference proceedings. (https://conferences.oreilly.com/artificial-intelligence/ai-ca-2018/public/schedule/detail/68831)\n",
    "\n",
    "The notebook uses Keras, a popular well-documented open source deep learning for implementing the LSTM model. To use Keras on Amazon SageMaker, we use the built-in Tensorflow environment that includes support for Keras.  This not only simplifies the development process, it also allows you to use standard Amazon SageMaker features like script mode.  The notebook also shows how to run the same Keras code on Amazon SageMaker that you run on your local machine, using script mode.\n",
    "\n",
    "We will follow this sequence in this notebook\n",
    "\n",
    "1. Examine and analyze the data\n",
    "2. Execute the python script that builds and trains the LSTM model using Keras. \n",
    "3. Train and deploy using Amazon SageMaker's 'local' mode\n",
    "4. Train on a training cluster of machine learning instances managed by Amazon SageMaker\n",
    "5. Deploy the trained model\n",
    "6. Execute inferences against the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the Python libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next specify : \n",
    "1. The IAM role arn used to give training and hosting access to your data. For this notebook, this is the same role you associated with this notebook instance.\n",
    "2. The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.  For this notebook, we will use the default S3 bucket associated with the sagemaker session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "s3_bucket = sess.default_bucket()\n",
    "print(\"s3_bucket is : \", s3_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Our data consists of a sequence of customer events and the trained model will predict the next sequence of the customer events. \n",
    "\n",
    "In this particular example, sequence of the customer events is the names of the TV channels viewed.  Let's say a company is running advertisements for a specific product or a new store location in these channels.  From the customer touchpoints, that capture their TV viewing behavior, the goal is to predict the next sequence of touchpoints.  The predicted sequence of touchpoints may include the next sequence of channels the customer will view and customer conversion behavior.  Conversion of the customer could mean that the customer visited the new store location or purchased a product being advertised on the TV channels. This specific event of interest is represented by the word 'visit' in our data set.\n",
    "\n",
    "<img src=\"../images/CustomerTouchPpoints.png\">\n",
    "\n",
    "Data is available in the \"data.txt\" file that accompanies this notebook.  It has been downloaded from https://github.com/shinchan75034/LSTM_TouchPoint. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the data from data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=\"data.txt\"\n",
    "\n",
    "with open(data_file, 'r') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    data = np.array(lines)\n",
    "#print(\"Data : \", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the data by viewing the first few elements of the data. \n",
    "Data is expected to have the structure of \n",
    "    **customer_id, sequence of input events and sequence of target events** each seperated by a **tab** character \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Show the first few lines of the data, to verify the data structure \n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training (80%) and test (20%) data sets\n",
    "Training data set is split into training and validation within the python script customer_event_prediction_lstm_keras_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split data into training (80%), test (20%) data sets\n",
    "\n",
    "train_dat, test_dat = train_test_split(data,test_size=0.2)\n",
    "\n",
    "#Verify training and test data set sizes\n",
    "print(\"\\nTraining Data Size \", len(train_dat))\n",
    "print(\"Test Data Size \", len(test_dat))\n",
    "        \n",
    "#Convert training and test data sets to lists\n",
    "train_lines = list(train_dat)\n",
    "test_lines = list(test_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define utility methods to create corpus, split lines into input & target and encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the corpus dictionary\n",
    "def create_corpus_dict(word_list):\n",
    "  token_index = dict(\n",
    "    [(word, i) for i, word in enumerate(word_list)])\n",
    "  return token_index\n",
    "\n",
    "#Split a given line into input and target\n",
    "def split_input_and_target(line_list):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    try:\n",
    "\n",
    "        for line in line_list:\n",
    "            _, input_text, target_text = line.split('\\t')\n",
    "            # We use \"tab\" as the \"start sequence\" character\n",
    "            # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "            target_text = '<start>' + \" \" + target_text + \" \" + '<stop>' \n",
    "            input_texts.append(input_text)\n",
    "            target_texts.append(target_text)\n",
    "            \n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    return input_texts, target_texts\n",
    "\n",
    "## Method to encode data\n",
    "def encode_data(input_texts,target_texts, input_vocab, target_vocab, input_corpus, target_corpus) :\n",
    "    \n",
    "    #Get the array/list length/counts\n",
    "    # input and target may have different vocab and different token count.\n",
    "    input_vocab = sorted(list(input_vocab))\n",
    "    target_vocab = sorted(list(target_vocab))\n",
    "    num_encoder_tokens = len(input_vocab)\n",
    "    num_decoder_tokens = len(target_vocab)\n",
    "    max_encoder_seq_length = max([len(txt.split()) for txt in input_texts]) # number of words in each string.  Use max length to make all sequences same size.\n",
    "    max_decoder_seq_length = max([len(txt.split()) for txt in target_texts])\n",
    "    \n",
    "    #Create zero encoded/decoder arrays of correct size\n",
    "    encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "    decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "    decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "    \n",
    "    #Now update the encoded/decoded arrays with 1.\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, word in enumerate(input_text.split()):\n",
    "            encoder_input_data[i, t, input_corpus[word]] = 1.\n",
    "        for t, word in enumerate(target_text.split()):\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t, target_corpus[word]] = 1.\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_corpus[word]] = 1.\n",
    "    \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define utility methods to upload to and read from S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3') \n",
    "\n",
    "##Save ndarray to file, upload to S3\n",
    "def upload_ndarray_to_s3(encoder_input_data, s3_prefix):\n",
    "    local_file = 'encoded_data.npy'\n",
    "    np.save(local_file, encoder_input_data) \n",
    "    s3.Bucket(s3_bucket).upload_file(local_file, s3_prefix)\n",
    "    \n",
    "    \n",
    "### Read ndarray from S3\n",
    "def read_ndarray_from_s3(s3_prefix):\n",
    "    local_file_downloaded = 'downloaded_encoder_data.npy'\n",
    "    s3.Bucket(s3_bucket).download_file(s3_prefix, local_file_downloaded)\n",
    "    downloaded_encoder_input_data = np.load(local_file_downloaded)\n",
    "    return downloaded_encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up all data to build a corpus\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_words = set()\n",
    "target_words = set()\n",
    "\n",
    "for line in lines:\n",
    "      try:\n",
    "        #print (\"line :\", line)\n",
    "        #Split each line into input text and target text.  Ignore the user_id (???)\n",
    "        _, input_text, target_text = line.split(\"\\t\")\n",
    "        #print(\"input_text :\", input_text, \" output text : \", target_text)\n",
    "\n",
    "\n",
    "        # We use \"tab\" as the \"start sequence\" character\n",
    "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "        #Update target text to include <start> and <stop> tokens\n",
    "        target_text = '<start>' + \" \" + target_text + \" \" + '<stop>'   \n",
    "\n",
    "\n",
    "        #Append input_texts and target_texts\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "\n",
    "        #Split the input_text and target_text into words and populate the input_words and target_words\n",
    "        for word in input_text.split():\n",
    "            if word not in input_words:\n",
    "                input_words.add(word)\n",
    "        for word in target_text.split():\n",
    "            if word not in target_words:\n",
    "                target_words.add(word)\n",
    "      except:\n",
    "        pass\n",
    "\n",
    "print(\"Number of input words \", len(input_words))\n",
    "print(\"Number of target words \", len(target_words))  #Should be two more than input words, since we added <start> and <stop>\n",
    "    \n",
    "    \n",
    "#Build the vocabulary.  Here it is simply union of the input and target words.\n",
    "vocab = list(set(input_words).union(set(target_words)))\n",
    "print(\"Vocab size \", len(vocab))\n",
    "    \n",
    "corpus_dict = create_corpus_dict(vocab)\n",
    "print(\"corpus_dict size \", len(corpus_dict))\n",
    "    \n",
    "# split each set of lines into input and target separately.\n",
    "train_input_texts, train_target_texts  = split_input_and_target(train_lines)\n",
    "#validation_input_texts, validation_target_texts  = split_input_and_target(validation_lines)\n",
    "test_input_texts, test_target_texts  = split_input_and_target(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encode training data and persist to S3.  This encoded data is used for training.\n",
    "train_encoder_input_data, train_decoder_input_data, train_decoder_target_data = encode_data(train_input_texts,train_target_texts, vocab,vocab, corpus_dict, corpus_dict)\n",
    "\n",
    "upload_ndarray_to_s3(train_encoder_input_data, \"train/train_encoder_input_data.npy\")\n",
    "upload_ndarray_to_s3(train_decoder_input_data, \"train/train_decoder_input_data.npy\")\n",
    "upload_ndarray_to_s3(train_decoder_target_data, \"train/train_decoder_target_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encode test data and persist to S3.  This encoded data is later on used for making inferences against the deployed model.\n",
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = encode_data(test_input_texts,test_target_texts, vocab,vocab, corpus_dict, corpus_dict)\n",
    "\n",
    "upload_ndarray_to_s3(test_encoder_input_data, \"test/test_encoder_input_data.npy\")\n",
    "upload_ndarray_to_s3(test_decoder_input_data, \"test/test_decoder_input_data.npy\")\n",
    "upload_ndarray_to_s3(test_decoder_target_data, \"test/test_decoder_target_data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define utility method to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use the predictor passed in for making predictions on the test data at 'prediction_index'\n",
    "\n",
    "stop_word_list = ['<start>', '<stop>']\n",
    "\n",
    "def predict(predictor, prediction_index):\n",
    "    \n",
    "    #print(\"encode input data type : \" , type(test_encoder_input_data[prediction_index]), \"encode input data size : \" , len(test_encoder_input_data[prediction_index ]))\n",
    "    predictions_from_model = predictor.predict({'encoder_input_data' : test_encoder_input_data[prediction_index], \n",
    "                                          'decoder_input_data' : test_decoder_input_data[prediction_index]})\n",
    "    \n",
    "    output_tokens = np.asarray(predictions_from_model['predictions'])\n",
    "    integer_list = output_tokens.argmax(axis=2)\n",
    "\n",
    "    # Reassign variables for convenience\n",
    "    input_token_index = corpus_dict\n",
    "    #target_token_index = corpus_dict\n",
    "    \n",
    "    # Reverse-lookup token index to decode sequences back to something readable.\n",
    "    reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "    #reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "    translated_array = np.vectorize(reverse_input_char_index.get)(integer_list) \n",
    "  \n",
    "    translated_list = translated_array.tolist()\n",
    "    \n",
    "    predicted_sequence = translated_list[0]\n",
    "    \n",
    "    ##Remove <start>, <stop> tokens from the predicted sequence\n",
    "    predicted_sequence_cleaned = [item for item in predicted_sequence if item not in stop_word_list]\n",
    "\n",
    "    return predicted_sequence_cleaned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the LSTM model, we follow this sequence to demonstrate multiple training options available :\n",
    "\n",
    "    1. Execute a python script right from this notebook.  The python script builds and trains the LSTM model using Keras.  This step allows you to verify that the python script will execute without any errors.\n",
    "    \n",
    "    2. Using the same python script as the entry point, we will then train (and deploy) using Amazon SageMaker 'local' mode. This step uses the \"Script\" mode with a prebuilt Tensorflow container launched on this very notebook instance.  This allows you to test the proper execution of the python script with Amazon SageMaker's prebuilt TensorFlow container, without the overhead of launching additional compute instances.\n",
    "       \n",
    "    3. Finally, we will train on a cluster of ML instances managed by Amazon SageMaker, thus creating a full scale training job.  This step is used to scale out the training job, with larger data sets and distributed training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat customer_event_prediction_lstm_keras_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the python script\n",
    "Verify that the python script will execute without any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete this directory the model is already persisted in this directory.\n",
    "!rm -rf /tmp/model/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python customer_event_prediction_lstm_keras_model.py --epochs 2 --batchsize 64 --modeldir '/tmp' --s3_bucket $s3_bucket "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train using the \"local\" mode\n",
    "In this mode, training happens on this notebook instance itself.  A \n",
    "Tensorflow container is launched, the python script is executed in the container and the model is persisted to the S3 bucket.\n",
    "Verify proper execution of the python script with Amazon SageMaker's prebuilt TensorFlow container, without the overhead of launching additional compute instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Tensorflow Estimator\n",
    "tf_estimator_local = TensorFlow(entry_point='customer_event_prediction_lstm_keras_model.py', \n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='local',\n",
    "                          framework_version='1.12', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={ 'epochs': 1, \n",
    "                                           'batch-size': 256,\n",
    "                                            'learning-rate': 0.01,\n",
    "                                           's3_bucket': s3_bucket }\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call fit on the estimator to kick off training.\n",
    "tf_estimator_local.fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deploy the predictor locally\n",
    "tf_predictor_local = tf_estimator_local.deploy(initial_instance_count=1,\n",
    "                         instance_type='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoke the predict method with the local predictor and the index of the test data items to get predictions for.\n",
    "predict (tf_predictor_local, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on a training cluster\n",
    "In this mode, training happens on new ML instances launced and managed by SageMaker.  Notice that the only parameter that is different is train_instance_type.  Instead of \"local\", we specify the type of the instance to be launched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Tensorflow Estimator\n",
    "tf_estimator_on_cluster = TensorFlow(entry_point='customer_event_prediction_lstm_keras_model.py', \n",
    "                          role=role,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='ml.m5.xlarge',\n",
    "                          framework_version='1.12', \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={\n",
    "                              'epochs': 20,\n",
    "                              'batch-size': 256,\n",
    "                              'learning-rate': 0.01,\n",
    "                              's3_bucket': s3_bucket}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Call fit on the estimator to kick off training. This takes approximately 13 minutes\n",
    "tf_estimator_on_cluster.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "Next deploy the model.  This step hosts the model on ML instance(s) managed by Amazon Sagemaker and return an \"Endpoint\" that will be used to run inference against in the \"Predict\" step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deploy the model trained on the cluster managed by Amazon SageMaker\n",
    "## This takes approximately 9 minutes\n",
    "tf_endpoint_name = 'customer-event-prediction-lstm'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "tf_predictor = tf_estimator_on_cluster.deploy(initial_instance_count=1,\n",
    "                         instance_type='ml.c5.large',        \n",
    "                         endpoint_name=tf_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Now let's use the deployed model for predictions.  Use the test_encoder_input_data and test_decoder_input_data to make the predictions.  Once predictions are done, calculate the confusion matrix and other model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Predict output for a single element (represented by the index) in the text target using the hosted predictor.\n",
    "predicted_sequence = predict(tf_predictor,0)\n",
    "print(\"predicted sequence \", predicted_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now predict for multiple elements in the text target \n",
    "all_predictions = []\n",
    "for i in range(0,len(test_encoder_input_data.tolist())):\n",
    "    predicted_sequence = predict(tf_predictor, i)\n",
    "    all_predictions.append(predicted_sequence)\n",
    "        \n",
    "print(\"Total number of predictions \", len(all_predictions))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate true/false positives/negative\n",
    "true_positives = 0\n",
    "true_negatives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for i in range(0,len(test_target_texts)):\n",
    "    if (\"visit\" in all_predictions[i] and \"visit\" in test_target_texts[i]):\n",
    "        true_positives = true_positives + 1\n",
    "    elif (\"visit\" in all_predictions[i] and \"visit\" not in test_target_texts[i]):   \n",
    "        false_positives = false_positives + 1\n",
    "    elif (\"visit\" not in all_predictions[i] and \"visit\" not in test_target_texts[i]):   \n",
    "        true_negatives = true_negatives + 1 \n",
    "    elif (\"visit\" not in all_predictions[i] and \"visit\" in test_target_texts[i]):   \n",
    "        false_negatives = false_negatives + 1     \n",
    "        \n",
    "print(\"true_positives : \", true_positives)  \n",
    "print(\"false_positives : \", false_positives) \n",
    "print(\"true_negatives : \", true_negatives) \n",
    "print(\"false_negatives : \", false_negatives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Recall, Precision, F1 and show confusion matrix\n",
    "\n",
    "Recall = true_positives / (true_positives + false_negatives)\n",
    "Precision = true_positives / (true_positives + false_positives)\n",
    "F1 = 2*(Recall*Precision)/(Recall + Precision)\n",
    "\n",
    "print(\"Model Metrics \")\n",
    "print(\"\\tRecall : \", Recall)  \n",
    "print(\"\\tPrecision : \", Precision) \n",
    "print(\"\\tF1 : \", F1) \n",
    "\n",
    "\n",
    "print(\"\\n\\n========== Confusion Matrix ==========\")\n",
    "\n",
    "print(\"\\t\\tVisit\\tNo Visit\\n\")\n",
    "print(\"Visit\\t\\t\", true_positives, \"\\t\", false_positives, \"\\n\")\n",
    "print(\"NoVisit\\t\\t\", false_negatives, \"\\t\", true_negatives,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can see a reasonable recall and precision with this model, there is definitely room for improvement. To improve the model metrics, consider :\n",
    "\n",
    "1. Increasing the training dataset volume.\n",
    "2. Experimenting with hyperparameter tuning to identify the right combinations of hyperparameters (epochs, learning rate, batch_size etc).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Endpoint\n",
    "\n",
    "Run the cell below to delete the endpoint once you are done.Â¶\n",
    "Note that till the endpoint is deleted, you are incurring costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
